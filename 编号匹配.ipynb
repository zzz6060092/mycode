{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 处理初始数据集paper，提取关键字\n",
    "\n",
    "\n",
    "class paper(object):\n",
    "    def __init__(self):\n",
    "        self.name = []\n",
    "        self.affiliations = []\n",
    "data = [paper() for i in range(2500000)]#创建一个类，包含每一篇文章的作者名，机构，年份\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filename1 = r\"C:\\Users\\Huawei\\Desktop\\PY实验\\数据处理\\paper-washed_affiliations-展开缩写.txt\"\n",
    "def Handling_text(num,str):\n",
    "    if str[:2] == '#@':#名字提取\n",
    "        data[num].name = str[3:len(str)-1].split(';')\n",
    "    if str[:2] == '#t':#年份提取\n",
    "        data[num].year = str[3:len(str)-1]\n",
    "        if data[num].year == '':\n",
    "            data[num].year = '0000'\n",
    "    if str[:2] == '#o':#机构提取\n",
    "        data[num].affiliations = str[3:len(str)-1].split(';')\n",
    "sum = -1\n",
    "with open(filename1,encoding='utf-8',errors='ignore') as f:\n",
    "    for line in f.readlines():\n",
    "        if(line[:6] == '#index'):\n",
    "            sum = sum+1\n",
    "        else :\n",
    "            Handling_text(sum,line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 计算hash值\n",
    "def calculate_hash_number(s):\n",
    "    hash = 5381\n",
    "    for c in s:\n",
    "        hash = (hash*33+ord(c))%147777777\n",
    "    return hash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def add_linked_list(name):#将名字信息加入链表中，方便查找\n",
    "    global Linked_P\n",
    "    hash_number = calculate_hash_number(name)\n",
    "    Linked_next.append(Linked_head[hash_number])\n",
    "    Linked_head[hash_number] = Linked_P\n",
    "    Linked_P += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#初始化hash值的位置并创建链表\n",
    "filename2 = r\"C:\\Users\\Huawei\\Desktop\\PY实验\\数据处理\\名字hash表（author_washed）.txt\"\n",
    "hash_answer = []#存储hash表中的内容\n",
    "Linked_head = [-1 for i in range(150000000)]#创建链表头文件\n",
    "Linked_next = []#链表next文件\n",
    "Linked_P = 0\n",
    "p = 0\n",
    "with open(filename2,encoding='utf-8') as f2:\n",
    "    for line in f2.readlines():\n",
    "        hash_answer.append(line[:len(line)-1].split('*!@$'))\n",
    "        hash_answer[p][4] = int(hash_answer[p][4])#讲hi转化为int类型\n",
    "        add_linked_list(hash_answer[p][2])#将名字的信息加入列表中\n",
    "        p += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\anaconda3\\lib\\site-packages\\fuzzywuzzy\\fuzz.py:11: UserWarning: Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning\n",
      "  warnings.warn('Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning')\n"
     ]
    }
   ],
   "source": [
    "#找寻名字的编号\n",
    "import re\n",
    "from fuzzywuzzy import fuzz\n",
    "def creat_new_name_data(a,b):#在新编号的名字中寻找是否有匹配信息，如果没有则作为新加入的信息，如果有直接返回之前已经编好的号码\n",
    "    global P1 \n",
    "    global P\n",
    "    tt = calculate_hash_number(data[a].name[b])\n",
    "    hash_answer.append([str(tt),str(P1),data[a].name[b],data[a].affiliations[b],1])\n",
    "    add_linked_list(data[a].name[b])\n",
    "    P1 += 1\n",
    "    P += 1\n",
    "    return P-1\n",
    "def find_capital(s):\n",
    "    lis = s.split(' ')\n",
    "    ls1 = []\n",
    "    for c in lis:\n",
    "        c.replace('-','')\n",
    "        if len(re.findall(r'[A-Z]',c)) > 0:\n",
    "            if c != 'The':\n",
    "                ls1.append(c)\n",
    "    s1 = ''.join(ls1)\n",
    "    return s1\n",
    "def match_affiliations(s1,s2):     #匹配机构信息，将paper中的作者机构信息用正则拆分成单个的单词去看这些单词在author中的机构中是否出现，\n",
    "                                   #如果匹配的占比达到一定的阈值，则判定是同一个人，不满足则判定不是同一个人\n",
    "    s1 = find_capital(s1)\n",
    "    cont = 0\n",
    "    owe = s2.split(';')\n",
    "    maxx = 0\n",
    "    for i in owe:\n",
    "        s3 = find_capital(i)\n",
    "        maxx = max(fuzz.token_set_ratio(s1,s3),maxx)\n",
    "    return maxx\n",
    "def find_name_number(t):#寻找名字的编号\n",
    "    global hi_or_pc\n",
    "    global the_number_of_same_name\n",
    "    answer = []\n",
    "    for j in range(len(data[t].name)):\n",
    "        tt = calculate_hash_number(data[t].name[j])\n",
    "        p = Linked_head[tt]\n",
    "        if p == -1:#如果在原author文件中找不到对应的人，则在新创建的信息中找\n",
    "            locat = creat_new_name_data(t,j)\n",
    "            answer.append(hash_answer[locat][1])\n",
    "            continue\n",
    "        maxx = 0\n",
    "        cont = 0\n",
    "        owe = []\n",
    "        locat = -1\n",
    "        while(p != -1):#寻找hash_answer中名字对应的人，并将其下标加入owe列表中\n",
    "            if data[t].name[j] == hash_answer[p][2]:\n",
    "                owe.append(p)\n",
    "            p = Linked_next[p]\n",
    "        if len(owe) == 0:#如果没有找到名字对应的人，那么就创建一个新的信息\n",
    "            locat = creat_new_name_data(t,j)\n",
    "            answer.append(hash_answer[locat][1])\n",
    "            continue\n",
    "        if len(owe) == 1 and int(hash_answer[owe[0]][1]) < 1712434:#如果有且仅有一个名字对应的人，那么就直接匹配给这个人\n",
    "            locat = owe[0]\n",
    "        if len(owe) > 1:#如果有两个及以上名字对应的人，即出现同名情况，那么需要匹配机构，如果paper中的这个人的机构信息缺失，那么直接把\n",
    "                        #他匹配给hi或pc最高的那个人。   \n",
    "            for i in owe:\n",
    "                if data[t].affiliations[j] != '-':\n",
    "                    MATCH_RATING = match_affiliations(data[t].affiliations[j],hash_answer[i][3])\n",
    "                    if MATCH_RATING > maxx and MATCH_RATING > 50:\n",
    "                        maxx = MATCH_RATING\n",
    "                        locat = i\n",
    "                else :\n",
    "                    if maxx <= hash_answer[i][4]:\n",
    "                        maxx = hash_answer[i][4]\n",
    "                        locat = i\n",
    "            if data[t].affiliations[j] == '-':\n",
    "                hi_or_pc += 1\n",
    "        if locat != -1 and len(owe) > 1 :\n",
    "            the_number_of_same_name += 1\n",
    "        if locat == -1:\n",
    "            locat = creat_new_name_data(t,j)\n",
    "            answer.append(hash_answer[locat][1])\n",
    "            continue\n",
    "        if locat > 171433:\n",
    "            hash_answer[locat][4] += 1\n",
    "        answer.append(hash_answer[locat][1])\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "899413   1986551\n"
     ]
    }
   ],
   "source": [
    "#找寻名字的编号\n",
    "from itertools import combinations\n",
    "ff = open(r\"C:\\Users\\Huawei\\Desktop\\PY实验\\数据处理\\编号匹配.txt\",\"w\",encoding = 'utf-8')\n",
    "name_number = []\n",
    "P = len(hash_answer)\n",
    "P1 = 1712434\n",
    "hi_or_pc = 0#记录通过hi或者pc来匹配的人的个数\n",
    "the_number_of_same_name = 0#记录总同名个数\n",
    "tt1 = (sum+1)\n",
    "for i in range(tt1):\n",
    "    name_number.append(find_name_number(i))\n",
    "for i in range(tt1):\n",
    "    for j in range(len(data[i].name)):\n",
    "        ff.write(str(name_number[i][j])+'*!@$')\n",
    "    ff.write(data[i].year+'\\n')\n",
    "print(hi_or_pc,' ',the_number_of_same_name)\n",
    "ff.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#提取合作关系\n",
    "ff = open(r\"C:\\Users\\Huawei\\Desktop\\PY实验\\数据处理\\合作关系提取（按年升序排列）.txt\",\"w\",encoding = 'utf-8')\n",
    "from itertools import combinations\n",
    "symbiosis = []\n",
    "for i in range(sum+1):\n",
    "    if len(data[i].name) == 1:\n",
    "        continue\n",
    "    year = data[i].year\n",
    "    for j in combinations(name_number[i],2):\n",
    "        lis = list(j)\n",
    "        if lis[0] > lis[1]:\n",
    "            lis[0],lis[1] = lis[1],lis[0]\n",
    "        lis.append(year)\n",
    "        symbiosis.append(lis)\n",
    "symbiosis.sort(key = lambda symbiosis:symbiosis[2])\n",
    "for i in range(len(symbiosis)):\n",
    "    ff.write(symbiosis[i][0]+'*!@$'+symbiosis[i][1]+'*!@$'+symbiosis[i][2]+'\\n')\n",
    "ff.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
